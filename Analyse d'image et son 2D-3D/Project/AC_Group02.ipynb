{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Generating Audio Reactive Visuals\n","\n","By:\n","* Daniel STULBERG HUF\n","* Lawson OLIVEIRA LIMA\n","* Lucas VITORIANO DE QUEIROZ LIRA"],"metadata":{"id":"TYtieE-dHmLC"}},{"cell_type":"markdown","source":["## 1. Introduction\n","\n","In this Applied Course, we want you to help us create and play with a music visualizer. More specifically, we want you to complete the development of a code which can generate a video that is responsive to the features of a soundtrack.\n","\n","During this process, we also want to introduce you to a powerful tool known as Generative Adversarial Networks (or simply GANs), which represents a class of Deep Learning frameworks that typically produces generative artworks. \n","\n","<br><strong>Acknowledgment:</strong> This file was adapted from the deep music visualizer created by <strong>Matt Siegelman</strong>. The article describing the tool can be read <a href=\"https://towardsdatascience.com/the-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9a\">here</a> and the repo containing the original script is available <a href=\"https://github.com/msieg/deep-music-visualizer\">here</a>. \n","\n","<br><strong>Note</strong>: Make sure that the runtime type of your colab is set to GPU."],"metadata":{"id":"bBmsovkALWon"}},{"cell_type":"markdown","source":["First of all, let's install and import all the libraries required to run this notebook."],"metadata":{"id":"0nyveoy2c0_1"}},{"cell_type":"code","source":["!pip install -q pytorch_pretrained_biggan\n","!pip install -q imageio==2.4.1\n","!pip install -q imageio-ffmpeg\n","\n","import sys\n","import librosa\n","import librosa.display\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import torch\n","import random\n","import moviepy.editor as mpy\n","from pytorch_pretrained_biggan import BigGAN, truncated_noise_sample\n","from tqdm import tqdm\n","from google.colab import files\n","\n","%matplotlib inline"],"metadata":{"id":"ZpU60smAc7_b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will also clone the GitHub repo of one of the members of our group in order to add some functions (you won't need to care about them)."],"metadata":{"id":"WOz_d3kFc8nK"}},{"cell_type":"code","source":["!git clone 'https://github.com/danielhuf/audio-reactive-visuals.git'\n","sys.path.append('/content/audio-reactive-visuals')\n","from utils import *"],"metadata":{"id":"0dmJEWfjdbE8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Of course, we also have to choose and then read the song that we want to visualize. We recommend that, for your first time, you keep with the song uploaded from the repo and pick only the first 30 seconds of the song in order to make the execution faster. But feel free to upload another mp3 file with the song of your choice."],"metadata":{"id":"T-rCb6ZcgmeV"}},{"cell_type":"code","source":["print('Reading audio') \n","song = '/content/audio-reactive-visuals/audios/up.mp3'   \n","y, sr = librosa.load(song, duration=30)   # Loading an audio file as a floating point time series with librosa\n","print('Audio successfully read') "],"metadata":{"id":"cbJpkZRdfzY2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Signal analysis\n","\n","In this section, we will process the audio by adjusting some parameters of the soundtrack. After that, we will perform a feature extraction to obtain a latent vector representation of the musical features at each timestep corresponding to a frame at a specific rate."],"metadata":{"id":"2BWXerxJLccz"}},{"cell_type":"markdown","source":["### 2.1. Setting audio parameters"],"metadata":{"id":"0VMzKnYJybp7"}},{"cell_type":"markdown","source":["The frame length is the number of audio samples per video frame. A low frame corresponds to a higher frame rate, which is suitable for visualizing very rapid music (but the rendering time should be longer). Conversely, a higher frame length corresponds to a lower frame rate, which cuts down runtime. "],"metadata":{"id":"jbfCL9TE2K-1"}},{"cell_type":"code","source":["frame_length = 256 # from 512 down means high quality (must be a multiple of 64)"],"metadata":{"id":"aoja3SM9yzNd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The pitch sensitivity controls the changes in the song pitch. A higher pitch sensitivy will make the shapes, textures and objects of the generated video to change more rapidly according to the music notes."],"metadata":{"id":"mun09WwA4DQr"}},{"cell_type":"code","source":["pitch_sensitivity = 220 # range [1, 299] ~int\n","pitch_sensitivity = (300 - pitch_sensitivity) * 512 / frame_length"],"metadata":{"id":"UiLytC9a4DeN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The tempo sensitivity controls the changes in the song volume and tempo (how fast or slow a piece of music is performed). A higher tempo sensitivity brings more movement to the generated video."],"metadata":{"id":"pPLg0hxZ4DpX"}},{"cell_type":"code","source":["tempo_sensitivity = 0.25 # range [0, 1] ~float\n","tempo_sensitivity = tempo_sensitivity * frame_length / 512"],"metadata":{"id":"4201bEcJ4D1f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.2. Extracting audio features\n","\n","Now that we've set the basic parameters of the audio, it is time to extract some of its features. In order to create a music visualizer, we will be playing mainly with two tools, the <strong>chromagram</strong> and the <strong>Mel spectogram</strong>."],"metadata":{"id":"KSY8C25HyiV3"}},{"cell_type":"markdown","source":["The chromagram is a representation that maps the whole spectral audio information into one octave, and each octave is split into 12 bins, each one representing a semitone of the song. The plot of the chromagram across the music time window allows us to observe how the representation's pitch content is spread over the 12 chroma bands and how much energy is present in each pitch class. "],"metadata":{"id":"hjp1v-ss9D7g"}},{"cell_type":"code","source":["# create chromagram of pitches X time points\n","chroma = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=frame_length)\n","\n","# sort pitches by overall power \n","chromasort = np.argsort(np.mean(chroma,axis=1))[::-1]\n","\n","# plot chromagram\n","img = librosa.display.specshow(chroma, y_axis='chroma', x_axis='time')\n","plt.title('Chromagram: ' + song.split('/')[-1])\n","plt.colorbar(img, label='Power')"],"metadata":{"id":"IiSa05gf7j26"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A spectogram allows us to visualize the energy of each frequency component of the song across time. Instead of a traditional spectogram, in which the frequencies are linearly distributed, we will be using the Mel spectogram. By scaling the frequency bins to Mel scales (in Decibel), we are simulating the human ear and therefore corresponding better to the human perception of a song. In the code below, besides creating the spectogram containing the overal power of the frequencies over time, we will also keep track of the gradient of such power. "],"metadata":{"id":"z_OIM_dWMrp3"}},{"cell_type":"markdown","source":["Complete the following line of code to compute a Mel spectogram of the uploaded audio. The spectogram should generate 128 Mel bands, a highest frequency of 8 kHz, and a hop length of value [```frame_length```]. (See documentation for the function below <a href=\"https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html\">here</a>)."],"metadata":{"id":"FXthlXxNAu1C"}},{"cell_type":"code","source":["# TO DO: create spectrogram\n","spec = librosa.feature.melspectrogram = (##############)"],"metadata":{"id":"NeFpvgcELE-x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<details>\n","  <summary> Solution </summary>\n","  <pre>\n","spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000, hop_length=frame_length)</pre>\n","</details>"],"metadata":{"id":"Ud9KN3fICGes"}},{"cell_type":"code","source":["# get mean power at each time point\n","specm = np.mean(spec,axis=0)\n","\n","# compute power gradient across time points\n","gradm = np.gradient(specm)\n","\n","# set max gradient to 1\n","gradm = gradm/np.max(gradm)\n","\n","# set negative gradient time points to zero \n","gradm = gradm.clip(min=0)\n","    \n","# normalize mean power between 0-1\n","specm = (specm-np.min(specm))/np.ptp(specm)\n","\n","# plot spectogram\n","librosa.display.specshow(librosa.power_to_db(spec, ref=np.max), y_axis='mel', fmax=8000, x_axis='time');\n","plt.title('Mel Spectrogram: ' + song.split('/')[-1]);\n","plt.colorbar(format='%+2.0f dB');"],"metadata":{"id":"6CbSNXFWAqG3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Okay, but what's all this for? We'll get back to this tools in a moment, we first need to introduce you to BigGan.  "],"metadata":{"id":"B-a7g4ERvaBf"}},{"cell_type":"markdown","source":["## 3. Visualization algorithm\n","\n","You may be wondering from where are we going to creat all the images, and the anser is: <strong>BigGAN</strong> and <strong>ImageNet</strong>."],"metadata":{"id":"xIKuDiaULcnA"}},{"cell_type":"markdown","source":["Briefly speaking, generative adversarial networks, or simply GANs, represent a class of machine learning frameworks trained by two neural networks competing in a zero-sum game: a generator creates new images learning from a database of images (ImageNet in our case), while a discriminator tries to classify the images as real or fake. At the end of this process, GANs are capable of generating high-quality synthetic images.  \n","\n","BigGAN is a state-of-the-art GAN architecture created by Google in 2018. The version that we'll use, which is ```BigGAN-Deep-256```, contains over 1 million features and 781 million parameters.\n","\n","We will be interested in handling with 1128 of this parameters, which are split between:\n","1.   A 1000-unit class vector corresponding to the 1000 classes of ImageNet images (you can consult them all on the file named ```/content/audio-reactive-visuals/imagenet_classes.txt```).\n","2.   A 128-unit noise vector with values ranging from -2 to 2 that controls the visual features of the generated images, introducing randomness and diversity to them.\n","\n","\n","\n"],"metadata":{"id":"CoKFmcrrHAa0"}},{"cell_type":"code","source":["# load pre-trained model\n","model = BigGAN.from_pretrained('biggan-deep-256')\n","\n","# set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# set number of classes to visualize\n","num_classes = 12 # range [1, 12] ~int\n","\n","# set classes (you can select a random set of classes, or define a numbered list of them by hand)\n","# NOTE: if you select a list of classes by hand, just comment the following 3 lines and also\n","# be sure that the length of the list of classes is the same as the number of classes previously set !!!\n","cls1000 = list(range(1000))\n","random.shuffle(cls1000)\n","classes = cls1000[:num_classes]\n","\n","# set truncation, i.e. variability of generated images. Higher truncation yields more variable images, while lower truncation yields simpler images\n","truncation = 1 # range [0.1, 1] ~float\n","\n","# set depth, i.e. the maximum value of the class vector units. Higher depth yields more thematically rich content, while lower depth yields more deep structures like human faces.\n","depth = 1 # range [0.01, 1] ~float\n","\n","# set smooth factor, i.e. the bin size for linearly interpolating between the means of the class vectors in order to smooth flutuations in the video.\n","# A higher smooth factor yields smoother results, while a lower smooth factor is suitable for visualizing fast music with rapid changes.\n","smooth_factor = 20 # range [10, 30] ~int\n","smooth_factor = int(smooth_factor * 512 / frame_length)"],"metadata":{"id":"AzT2KKa3ADxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Initializing input vectors')\n","\n","# initialize first class vector\n","cv1 = np.zeros(1000)\n","\n","for pi, p in enumerate(chromasort[:num_classes]):\n","    \n","    if num_classes < 12:\n","        cv1[classes[pi]] = chroma[p][np.min([np.where(chrow>0)[0][0] for chrow in chroma])]       \n","    else:\n","        cv1[classes[p]] = chroma[p][np.min([np.where(chrow>0)[0][0] for chrow in chroma])]\n","\n","# initialize first noise vector\n","nv1 = truncated_noise_sample(truncation=truncation)[0]\n","\n","# initialize list of class and noise vectors\n","class_vectors = [cv1]\n","noise_vectors = [nv1]\n","\n","# initialize previous vectors (will be used to track the previous frame)\n","cvlast = cv1\n","nvlast = nv1\n","\n","# initialize the direction of noise vector unit updates\n","update_dir = np.zeros(128)\n","\n","for ni, n in enumerate(nv1):\n","    if n < 0:\n","        update_dir[ni] = 1\n","    else:\n","        update_dir[ni] = -1\n","\n","# initialize noise unit update\n","update_last = np.zeros(128)\n","\n","print('Vectors successfully initialized') "],"metadata":{"id":"LjmP4MLyl2QC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Interpolating between classes and/or noises in the latent space may lead \n","to interesting results, generally explored by artists to build AI artworks. In our case, we will set BigGan to play with music.\n","\n","You do not need to understand deeply how the following algorithm works. What is important to keep in mind is that the algorithm syncs the pitch value with the class vector and the tempo value with the noise vector. Furthermore, at each time point of the song, the weights of the ImageNet classes of the class vector will be determined by the power of the pitches from the chromagram (as seen in the figure below). Independently, the change rate of the noise vector will determine the changes in tempo and volume of the song.\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*FKaaLad96Mhqsx4W_ShQTw.jpeg\" width=\"500px\" align=\"center\">\n","\n","Font: <a href=\"https://towardsdatascience.com/the-deep-music-visualizer-using-sound-to-explore-the-latent-space-of-biggan-198cd37dac9aa\">Matt Siegelman (Towards Data Science)</a>"],"metadata":{"id":"QD-14NGvLs_W"}},{"cell_type":"code","source":["print('\\nGenerating input vectors \\n')\n","\n","for i in tqdm(range(len(gradm))):   \n","\n","    # update jitter vector (property for avoiding noise repetitiveness) every 200 frames by setting ~half of noise vector units to lower sensitivity\n","    if i%200 == 0:\n","        jitters = new_jitters()\n","\n","    # get last noise vector\n","    nv1 = nvlast\n","\n","    # set noise vector update based on direction, tempo sensitivity, jitter, and combination of overall power and gradient of power\n","    update = np.array([tempo_sensitivity for k in range(128)]) * (gradm[i]+specm[i]) * update_dir * jitters \n","    \n","    # smooth the update with the previous update (to avoid overly sharp frame transitions)\n","    update = (update+update_last*3)/4\n","    \n","    # set last update\n","    update_last = update\n","        \n","    # update noise vector\n","    nv2 = nv1 + update\n","\n","    # append to noise vectors\n","    noise_vectors.append(nv2)\n","    \n","    # set last noise vector\n","    nvlast = nv2\n","                   \n","    # update the direction of noise units\n","    for ni, n in enumerate(nv2):\n","\n","      if n >= 2*truncation - tempo_sensitivity:\n","        update_dir[ni] = -1  \n","                    \n","      elif n < -2*truncation + tempo_sensitivity:\n","        update_dir[ni] = 1 \n","\n","    # get last class vector\n","    cv1 = cvlast\n","    \n","    # generate new class vector based on chromatic notes and tempo sensitivity\n","    cv2 = np.zeros(1000)\n","    for j in range(num_classes):\n","        \n","        cv2[classes[j]] = (cvlast[classes[j]] + ((chroma[chromasort[j]][i])/(pitch_sensitivity)))/(1+(1/((pitch_sensitivity))))\n","\n","    # if more than 6 classes, normalize new class vector between 0 and 1, else simply set max class val to 1\n","    if num_classes > 6:\n","        cv2 = normalize_cv(cv2)\n","    else:\n","        cv2 = cv2/np.max(cv2)\n","    \n","    # adjust depth    \n","    cv2 = cv2 * depth\n","    \n","    # this prevents rare bugs where all classes have the same value\n","    if np.std(cv2[np.where(cv2!=0)]) < 0.0000001:\n","        cv2[classes[0]] = cv2[classes[0]] + 0.01\n","\n","    # append new class vector\n","    class_vectors.append(cv2)\n","    \n","    # set last class vector\n","    cvlast = cv2\n","\n","\n","# interpolate between class vectors of bin size [smooth_factor] to smooth frames \n","class_vectors = smooth(class_vectors, smooth_factor)\n","\n","\n","# convert vectors to Tensor\n","noise_vectors = torch.Tensor(np.array(noise_vectors))      \n","class_vectors = torch.Tensor(np.array(class_vectors))"],"metadata":{"id":"7S_GAph6OLSg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Rendering final video\n","\n","In this section, we will finally generate the frames and then combine them into one single video."],"metadata":{"id":"RJoYiF_JLcwQ"}},{"cell_type":"markdown","source":["Before doing that, we first need to set some parameters."],"metadata":{"id":"hU4C0rLrkUiN"}},{"cell_type":"code","source":["# set batch size, i.e. size of the batch that BigGAN will use to generate the images\n","batch_size = 30\n","\n","# set number of frames per second based on the song duration, the frame length, and the batch size\n","frame_lim = int(np.floor(len(y)/sr*22050/frame_length/batch_size))\n","\n","# set output file name\n","outname = song.split('/')[-1][:-1] + '4'"],"metadata":{"id":"Q_PGd-Z63n2q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After that, for each frame of the video, the model will produce one image. In the end, the frames are all combined by using the ```FFmpeg``` library. We also add the accompanying audio to the final video."],"metadata":{"id":"WqYfepbEX_SV"}},{"cell_type":"code","source":["print('\\n\\nGenerating frames \\n')\n","\n","# send to CUDA if running on GPU\n","model = model.to(device)\n","noise_vectors = noise_vectors.to(device)\n","class_vectors = class_vectors.to(device)\n","\n","frames = []\n","\n","for i in tqdm(range(frame_lim)):\n","\n","    if (i+1)*batch_size > len(class_vectors):\n","        torch.cuda.empty_cache()\n","        break\n","    \n","    # get batch\n","    noise_vector = noise_vectors[i*batch_size:(i+1)*batch_size]\n","    class_vector = class_vectors[i*batch_size:(i+1)*batch_size]\n","\n","    # TO DO: call the model passing the noise vector, the class vector, and the truncation value in order to generate the images\n","    with torch.no_grad():\n","        output = model(###########)\n","\n","    output_cpu = output.cpu().data.numpy()\n","\n","    # convert to image array and add to frames\n","    for out in output_cpu:  \n","        im = np.array(toimage(out))\n","        frames.append(im)\n","        \n","    #empty cuda caches\n","    torch.cuda.empty_cache()\n","\n","#Save video  \n","aud = mpy.AudioFileClip(song, fps = 44100) \n","aud.duration = int(len(y)/sr)\n","\n","clip = mpy.ImageSequenceClip(frames, fps=22050/frame_length)\n","clip = clip.set_audio(aud)\n","clip.write_videofile(outname,audio_codec='aac')"],"metadata":{"id":"CH0HIwEn3J1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<details>\n","  <summary> Solution </summary>\n","  <pre>\n","output = model(noise_vector, class_vector, truncation)</pre>\n","</details>"],"metadata":{"id":"CoE5_J30EG2M"}},{"cell_type":"markdown","source":["If you want to check the final result, run the following line of code to visualize the generated mp4 file."],"metadata":{"id":"AK40EZfsoyPj"}},{"cell_type":"code","source":["show_video('/content/' + outname)"],"metadata":{"id":"ga8YRNy8oydb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. MCQ"],"metadata":{"id":"PNQUd1gWLc5x"}},{"cell_type":"markdown","source":["<b>1. What do the class and noise vectors mean in the BigGAN network?</b>\n","\n","<div>\n","  A. <input type=\"checkbox\">\n","  <label><strong>Class vector</strong>: the 12 pitches of the song\n","  <br>&nbsp &nbsp &nbsp &nbsp &nbsp <strong>Noise vector</strong>: variations of each song pitch</label>\n","</div>\n","\n","<br>\n","<div>\n","  B. <input type=\"checkbox\">\n","  <label><strong>Class vector</strong>: ImageNet groups of images\n","  <br>&nbsp &nbsp &nbsp &nbsp &nbsp <strong>Noise vector</strong>: randomness of the generated images</label>\n","</div>\n","\n","<br>\n","<div>\n","  C. <input type=\"checkbox\">\n","  <label><strong>Class vector</strong>: ImageNet groups of images\n","  <br>&nbsp &nbsp &nbsp &nbsp &nbsp <strong>Noise vector</strong>: visual features of generated images</label>\n","</div>\n","\n","<br>\n","<div>\n","  D. <input type=\"checkbox\">\n","  <label><strong>Class vector</strong>: visual features of generated images\n","  <br>&nbsp &nbsp &nbsp &nbsp &nbsp <strong>Noise vector</strong>: variations of each song pitch</label>\n","</div>\n","\n","<br>\n","<details>\n","  <summary> Answer </summary>\n","C\n","</details>"],"metadata":{"id":"2_tD0hIXKtvi"}},{"cell_type":"markdown","source":["<b>2. Which of these options represent parameters of the visualizer that we have implemented? (1 answer or more)</b>\n","\n","<div>\n","  A. <input type=\"checkbox\">\n","  <label>Tempo sensitivity</label>\n","</div>\n","\n","<div>\n","  B. <input type=\"checkbox\">\n","  <label>Equalization</label>\n","</div>\n","\n","<div>\n","  C. <input type=\"checkbox\">\n","  <label>Overdrive</label>\n","</div>\n","\n","<div>\n","  D. <input type=\"checkbox\">\n","  <label>Truncation</label>\n","</div>\n","\n","<br>\n","<details>\n","  <summary> Answer </summary>\n","AD\n","</details>"],"metadata":{"id":"a-lykZiiNT5b"}},{"cell_type":"markdown","source":["<b>3. What statement(s) is (are) true about the content of this notebook?</b>\n","\n","<div>\n","  A. <input type=\"checkbox\">\n","  <label>This audio reactive visualizer always produce the same visual output for the same sound input.</label>\n","</div>\n","\n","<div>\n","  B. <input type=\"checkbox\">\n","  <label>The algorithm is capable of automatically selecting ImageNet classes based on semantic associations with song lyrics.</label>\n","</div>\n","\n","<div>\n","  C. <input type=\"checkbox\">\n","  <label>The generated images are created by interpolating the values of the class and noise vectors in the latent space.</label>\n","</div>\n","\n","<div>\n","  D. <input type=\"checkbox\">\n","  <label>None of the above.</label>\n","</div>\n","\n","<br>\n","<details>\n","  <summary> Answer </summary>\n","C\n","</details>"],"metadata":{"id":"xiki9tsxPEn7"}},{"cell_type":"markdown","source":["<b>4. In which of these applications can an audio reactive visualizer be used?</b>\n","\n","<div>\n","  A. <input type=\"checkbox\">\n","  <label>Build a visualizer that responds to live music in real time.</label>\n","</div>\n","\n","<div>\n","  B. <input type=\"checkbox\">\n","  <label>Interface the class and noise vectors with neural activity to create deep music videos from the brain.</label>\n","</div>\n","\n","<div>\n","  C. <input type=\"checkbox\">\n","  <label>Meditation techniques that create a calming visual environment, complementing the soundscape and promoting relaxation.</label>\n","</div>\n","\n","<div>\n","  D. <input type=\"checkbox\">\n","  <label>All of the above.</label>\n","</div>\n","\n","<br>\n","<details>\n","  <summary> Answer </summary>\n","D\n","</details>"],"metadata":{"id":"woFMgg5wTRFg"}},{"cell_type":"markdown","source":["## 6. Bonus: Playground (and other cool examples)"],"metadata":{"id":"IrDO-046LdQt"}},{"cell_type":"markdown","source":["Now, it is up to you generate a deep music visualizer the way you want! As a reminder, here are all the parameters you that can change in this notebook:\n","\n","\n","*   ```song```\n","*   ```frame_length```\n","*   ```pitch_sensitivity```\n","*   ```tempo_sensitivity```\n","*   ```num_classes```\n","*   ```classes```\n","*   ```truncation```\n","*   ```depth```\n","*   ```smooth_factor```\n"],"metadata":{"id":"HBrsyrUmpbsz"}},{"cell_type":"markdown","source":["We have also created some cool examples that you can check out by yourself."],"metadata":{"id":"ivbZ66DUwUAD"}},{"cell_type":"code","source":["show_video('/content/audio-reactive-visuals/creations/bee_thoven.mp4')            # classes = [309]"],"metadata":{"id":"V03km7x4rENf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_video('/content/audio-reactive-visuals/creations/strawberry_beatles.mp4')    # classes = [949]"],"metadata":{"id":"PiDzL0lGL4Fj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_video('/content/audio-reactive-visuals/creations/french_pride.mp4')          # classes = [975, 978]"],"metadata":{"id":"Frrn83JYL4mz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_video('/content/audio-reactive-visuals/creations/acid_rave.mp4')             # classes = [947]"],"metadata":{"id":"CvinJRRCL4yR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_video('/content/audio-reactive-visuals/creations/otaku_bird.mp4')            # classes = [13, 14]"],"metadata":{"id":"BYcihrwML46N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lastly, you can also take a look at what the original authors have created using this algorithm. \n","\n","* <a href=\"https://instagram.com/deep_music_visualizer?igshid=YmMyMTA2M2Y=\">@deep_music_visualizer (Matt Siegelman)</a>\n","* <a href=\"https://instagram.com/lucidsonicdreams?igshid=YmMyMTA2M2Y=\">@lucidsonicdreams (Mikael Alafriz)</a>"],"metadata":{"id":"f_xBJUGwzb0T"}}]}